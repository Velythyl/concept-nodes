# Ollama local LLM configuration (for local inference)
_target_: langchain_ollama.ChatOllama
model: llama3.2
temperature: 0.7
num_predict: 1024
base_url: http://localhost:11434
